<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python 数据类型]]></title>
    <url>%2F2017%2F12%2F04%2Fpython2%2F</url>
    <content type="text"><![CDATA[数据类型在Python中，能够直接处理的数据类型有以下几种： 基本数据类型整数Python可以处理任意大小的整数，当然包括负整数，在程序中的表示方法和数学上的写法一模一样，例如：1，100，-8080，0，等等。计算机由于使用二进制，所以，有时候用十六进制表示整数比较方便，十六进制用0x前缀和0-9，a-f表示，例如：0xff00，0xa5b4c3d2，等等。 浮点数浮点数也就是小数，之所以称为浮点数，是因为按照科学记数法表示时，一个浮点数的小数点位置是可变的，浮点数可以用数学写法，如1.23，3.14，-9.01，等等。但是对于很大或很小的浮点数，就必须用科学计数法表示，把10用e替代，1.23x109就是1.23e9，或者12.3e8，0.000012可以写成1.2e-5，等等。 字符串字符串是以’’或””括起来的任意文本，比如&#39;abc&#39;，&quot;xyz&quot;等等。请注意，&#39;&#39;或&quot;&quot;本身只是一种表示方式，不是字符串的一部分，因此，字符串&#39;abc&#39;只有a，b，c这3个字符。如果&#39;本身也是一个字符，那就可以用&quot;&quot;括起来，比如&quot;I&#39;m OK&quot;包含的字符是I，&#39;，m，空格，O，K这6个字符。如果字符串内部既包含&#39;又包含&quot;怎么办？可以用转义字符\来标识，比如：&#39;I\&#39;m \&quot;OK\&quot;!&#39;表示的字符串内容是：I&#39;m &quot;OK&quot;! 布尔值布尔值和布尔代数的表示完全一致，一个布尔值只有True、False两种值，要么是True，要么是False，在Python中，可以直接用True、False表示布尔值（请注意大小写），也可以通过布尔运算计算出来：12345678print TrueTrueprint FalseFalseprint 3 &gt; 2Trueprint 3 &gt; 5False 布尔值可以用and、or和not运算。 空值空值是Python里一个特殊的值，用None表示。None不能理解为0，因为0是有意义的，而None是一个特殊的空值。 变量变量名必须是大小写英文、数字和_的组合，且不能用数字开头 ##数据类型进阶 列表（list）Python内置的一种数据类型是列表：list。list是一种有序的集合，可以随时添加和删除其中的元素。声明list：list=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]用索引来访问list中每一个位置的元素，记得索引是从0开始的例如list [0]如果要取最后一个元素，除了计算索引位置外，还可以用-1做索引，直接获取最后一个元素：list[-1]list是一个可变的有序表，所以，可以往list中追加元素到末尾：list.append([指定位置]&#39;aaa&#39;)要删除list的元素，用pop()方法：list.pop([指定位置默认为最后])要把某个元素替换成别的元素，可以直接赋值给对应的索引位置,list里面的元素的数据类型也可以不同。list元素也可以是另一个list,例如：s = [&#39;python&#39;, &#39;java&#39;, [&#39;asp&#39;, &#39;php&#39;], &#39;scheme&#39;] 元祖（tuple）另一种有序列表叫元组：tuple。tuple和list非常类似，但是tuple一旦初始化就不能修改声明元祖t=（1，2）只有1个元素的tuple定义时必须加一个逗号,，来消除歧义：t=(1,)最后来看一个“可变的”tuple：123t = (&apos;a&apos;, &apos;b&apos;, [&apos;A&apos;, &apos;B&apos;])t[2][0] = &apos;X&apos;t[2][0] = &apos;X&apos; 字典（dict）Python内置了字典：dict的支持，dict全称dictionary，在其他语言中也称为map，使用键-值（key-value）存储，具有极快的查找速度。声明dict d = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3} setset和dict类似，也是一组key的集合，但不存储value。由于key不能重复，所以，在set中，没有重复的key。要创建一个set，需要提供一个list作为输入集合:声明 set s = set([1,2,3])set可以看成数学意义上的无序和无重复元素的集合，因此，两个set可以做数学意义上的交集、并集等操作：123456print s1 = set([1, 2, 3])print s2 = set([2, 3, 4])print s1 &amp; s2set([2, 3])print s1 | s2set([1, 2, 3, 4])]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>-python 语法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 安装]]></title>
    <url>%2F2017%2F12%2F04%2FPython%2F</url>
    <content type="text"><![CDATA[python 环境准备安装 Python 首先从Python的官方网站python.org下载最新的2.7版本，网速慢的同学请移步国内镜像。 然后，运行下载的MSI安装包，在选择安装组件的一步时，勾上所有的组件： 特别要注意选上pip和Add python.exe to Path，然后一路点“Next”即可完成安装。 默认会安装到C:\Python27目录下，然后打开命令提示符窗口，敲入python后，会出现两种情况。情况一：看到上面的画面，就说明Python安装成功！你看到提示符&gt;&gt;&gt;就表示我们已经在Python交互式环境中了，可以输入任何Python代码，回车后会立刻得到执行结果。现在，输入exit()并回车，就可以退出Python交互式环境（直接关掉命令行窗口也可以！）。情况二：得到一个错误： ‘python’不是内部或外部命令，也不是可运行的程序或批处理文件。 这是因为Windows会根据一个Path的环境变量设定的路径去查找python.exe，如果没找到，就会报错。如果在安装时漏掉了勾选Add python.exe to Path，那就要手动把python.exe所在的路径C:\Python27添加到Path中。 如果你不知道怎么修改环境变量，建议把Python安装程序重新运行一遍，记得勾上Add python.exe to Path。安装 pycharm 首先从网站下载pycharm: http://www.jetbrains.com/pycharm/download/#section=windows，进入之后如下图，根据自己电脑的操作系统进行选择，对于windows系统选择图中红色圈中的区域。 下载完成之后如下图： 直接双击下载好的exe文件进行安装,点击Next进入下一步： 点击Install进行安装 安装完成后出现下图界面，点级Finish结束安装使用pycharm 单击桌面上的pycharm图标，进入到pycharm中 我们选择第二个，然后点击Ok 点击Accept进入下一步： 点击上图中的ok进入下一步：至此，安装完成]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python 安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件名]]></title>
    <url>%2F2017%2F12%2F04%2F%E6%96%87%E4%BB%B6%E5%90%8D%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[java]]></title>
    <url>%2F2017%2F12%2F04%2FJava%2F</url>
    <content type="text"></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>-java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫认知]]></title>
    <url>%2F2017%2F12%2F04%2F%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[综述大家好，最近博主在学习Python，学习期间也遇到一些问题，获得了一些经验，在此将自己的学习系统地整理下来，如果大家有兴趣学习爬虫的话，可以将这些文章作为参考，也欢迎大家一共分享学习经验。 Python版本:2.7，Python 3请另寻其他博文。 根据我的经验，要学习Python爬虫，我们要学习的共有以下几点： Python基础知识 Python中urllib和urllib2库的用法 Python正则表达式 Python爬虫框架Scrapy Python爬虫更高级的功能 Python基础学习首先，我们要用Python写爬虫，肯定要了解Python的基础吧，万丈高楼平地起，不能忘啦那地基，哈哈，那么我就分享一下自己曾经看过的一些Python教程，小伙伴们可以作为参考。 慕课网Python教程曾经有一些基础的语法是在慕课网上看的，上面附有一些练习，学习完之后可以作为练习，感觉效果还是蛮不错的，不过稍微遗憾的是内容基本上都是最基础的，入门开始的话，就这个吧 学习网址：https://www.imooc.com/course/list?c=python 廖雪峰Python教程后来，我发现了廖老师的Python教程，讲的那是非常通俗易懂哪，感觉也是非常不错，大家如果想进一步了解Python就看一下这个吧。 学习网址：https://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000 Python urllib和urllib2 库的用法urllib和urllib2库是学习Python爬虫最基本的库，利用这个库我们可以得到网页的内容，并对内容用正则表达式提取分析，得到我们想要的结果。这个在学习过程中我会和大家分享的。 Python 正则表达式Python正则表达式是一种用来匹配字符串的强有力的武器。它的设计思想是用一种描述性的语言来给字符串定义一个规则，凡是符合规则的字符串，我们就认为它“匹配”了，否则，该字符串就是不合法的。这个在后面的博文会分享的。 爬虫框架Scrapy如果你是一个Python高手，基本的爬虫知识都已经掌握了，那么就寻觅一下Python框架吧，我选择的框架是Scrapy框架。这个框架有什么强大的功能呢？下面是它的官方介绍： HTML, XML源数据 选择及提取 的内置支持提供了一系列在spider之间共享的可复用的过滤器(即 Item Loaders)，对智能处理爬取数据提供了内置支持。通过 feed导出 提供了多格式(JSON、CSV、XML)，多存储后端(FTP、S3、本地文件系统)的内置支持提供了media pipeline，可以 自动下载 爬取到的数据中的图片(或者其他资源)。高扩展性。您可以通过使用 signals ，设计好的API(中间件, extensions, pipelines)来定制实现您的功能。内置的中间件及扩展为下列功能提供了支持:cookies and session 处理HTTP 压缩HTTP 认证HTTP 缓存user-agent模拟robots.txt爬取深度限制针对非英语语系中不标准或者错误的编码声明, 提供了自动检测以及健壮的编码支持。支持根据模板生成爬虫。在加速爬虫创建的同时，保持在大型项目中的代码更为一致。详细内容请参阅 genspider 命令。针对多爬虫下性能评估、失败检测，提供了可扩展的 状态收集工具 。提供 交互式shell终端 , 为您测试XPath表达式，编写和调试爬虫提供了极大的方便提供 System service, 简化在生产环境的部署及运行内置 Web service, 使您可以监视及控制您的机器内置 Telnet终端 ，通过在Scrapy进程中钩入Python终端，使您可以查看并且调试爬虫Logging 为您在爬取过程中捕捉错误提供了方便支持 Sitemaps 爬取具有缓存的DNS解析器 官方文档：http://doc.scrapy.org/en/latest/ 爬虫基础了解什么是爬虫爬虫，即网络爬虫，大家可以理解为在网络上爬行的一直蜘蛛，互联网就比作一张大网，而爬虫便是在这张网上爬来爬去的蜘蛛咯，如果它遇到资源，那么它就会抓取下来。想抓取什么？这个由你来控制它咯。比如它在抓取一个网页，在这个网中他发现了一条道路，其实就是指向网页的超链接，那么它就可以爬到另一张网上来获取数据。这样，整个连在一起的大网对这之蜘蛛来说触手可及，分分钟爬下来不是事儿。 浏览网页的过程在用户浏览网页的过程中，我们可能会看到许多好看的图片，比如 http://image.baidu.com/ ，我们会看到几张的图片以及百度搜索框，这个过程其实就是用户输入网址之后，经过DNS服务器，找到服务器主机，向服务器发出一个请求，服务器经过解析之后，发送给用户的浏览器 HTML、JS、CSS 等文件，浏览器解析出来，用户便可以看到形形色色的图片了。 URL的含义URL，即统一资源定位符，也就是我们说的网址，统一资源定位符是对可以从互联网上得到的资源的位置和访问方法的一种简洁的表示，是互联网上标准资源的地址。互联网上的每个文件都有一个唯一的URL，它包含的信息指出文件的位置以及浏览器应该怎么处理它。URL的格式由三部分组成： ①第一部分是协议(或称为服务方式)。 ②第二部分是存有该资源的主机IP地址(有时也包括端口号)。 ③第三部分是主机资源的具体地址，如目录和文件名等。爬虫爬取数据时必须要有一个目标的URL才可以获取数据，因此，它是爬虫获取数据的基本依据，准确理解它的含义对爬虫学习有很大帮助。 环境的配置学习Python，当然少不了环境的配置，最初我用的是Sublime，不过发现它没有提示功能实在是太弱了，于是，在Windows下我用了 PyCharm.]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Urllib库]]></title>
    <url>%2F2017%2F12%2F04%2F%E7%88%AC%E8%99%AB2%2F</url>
    <content type="text"><![CDATA[urllib库的使用扒一个网页怎样扒网页呢？其实就是根据URL来获取它的网页信息，虽然我们在浏览器中看到的是一幅幅优美的画面，但是其实是由浏览器解释才呈现出来的，实质它 是一段HTML代码，加 JS、CSS，如果把网页比作一个人，那么HTML便是他的骨架，JS便是他的肌肉，CSS便是它的衣服。所以最重要的部分是存在于HTML中的，下面我 们就写个例子来扒一个网页下来。在pycharm里新建一个.py的文件名字叫firstparser.py1234import urllib2response = urllib2.urlopen(&quot;http://www.baidu.com&quot;)print response.read() 是的你没看错，真正的程序就两行，右键选择 run ，查看运行结果，感受一下:看，这个网页的源码已经被我们扒下来了，是不是很酸爽？ 分析扒网页的方法那么我们来分析这两行代码，第一行: response = urllib2.urlopen(&quot;http://www.baidu.com&quot;) 首先我们调用的是urllib2库里面的urlopen方法，传入一个URL，这个网址是百度首页，协议是HTTP协议，当然你也可以把HTTP换做FTP,FILE,HTTPS 等等，只是代表了一种访问控制协议，urlopen一般接受三个参数，它的参数如下： urlopen(url, data, timeout) 第一个参数url即为URL，第二个参数data是访问URL时要传送的数据，第三个timeout是设置超时时间。 第二三个参数是可以不传送的，data默认为空None，timeout默认为 socket._GLOBAL_DEFAULT_TIMEOUT 第一个参数URL是必须要传送的，在这个例子里面我们传送了百度的URL，执行urlopen方法之后，返回一个response对象，返回信息便保存在这里面。 print response.read() response对象有一个read方法，可以返回获取到的网页内容。 构造Requset其实上面的urlopen参数可以传入一个request请求,它其实就是一个Request类的实例，构造时需要传入Url,Data等等的内容。比如上面的两行代码，我们可以这么改写:12345import urllib2request = urllib2.Request(&quot;http://www.baidu.com&quot;)response = urllib2.urlopen(request)print response.read() 运行结果是完全一样的，只不过中间多了一个request对象，推荐大家这么写，因为在构建请求时还需要加入好多内容，通过构建一个request，服务器响应请求得到应答，这样显得逻辑上清晰明确。 POST和GET数据传送上面的程序演示了最基本的网页抓取，不过，现在大多数网站都是动态网页，需要你动态地传递参数给它，数据传送分为POST和GET两种方式: POST我们传送的数据就是这个参数data，下面演示一下POST方式。123456789import urllibimport urllib2values = &#123;&quot;username&quot;:&quot;1016903103@qq.com&quot;,&quot;password&quot;:&quot;XXXX&quot;&#125;data = urllib.urlencode(values)url = &quot;https://passport.csdn.net/account/login?from=http://my.csdn.net/my/mycsdn&quot;request = urllib2.Request(url,data)response = urllib2.urlopen(request)print response.read() 在此只是说明登录的原理。我们需要定义一个字典，名字为values，参数我设置了username和 password，下面利用urllib的urlencode方法将字典编码，命名为data，构建request时传入两个参数，url和data，运 行程序，即可实现登陆，返回的便是登陆后呈现的页面内容。当然你可以自己搭建一个服务器来测试一下。 GET方式至于GET方式我们可以直接把参数写到网址上面，直接构建一个带参数的URL出来即可。123456789101112import urllibimport urllib2values=&#123;&#125;values[&apos;username&apos;] = &quot;1016903103@qq.com&quot;values[&apos;password&apos;]=&quot;XXXX&quot;data = urllib.urlencode(values)url = &quot;http://passport.csdn.net/account/login&quot;geturl = url + &quot;?&quot;+datarequest = urllib2.Request(geturl)response = urllib2.urlopen(request)print response.read() 你可以print geturl，打印输出一下url，发现其实就是原来的url加？然后加编码后的参数1http://passport.csdn.net/account/login?username=1016903103%40qq.com&amp;password=XXXX urllib库的扩展设置Headers有些网站不会同意程序直接用上面的方式进行访问，如果识别有问题，那么站点根本不会响应，所以为了完全模拟浏览器的工作，我们需要设置一些Headers 的属性。 首先，打开我们的浏览器，调试浏览器F12，我用的是Chrome，打开网络监听经过多次请求之后，网页的骨架和肌肉全了，整个网页的效果也就出来了。 拆分这些请求，我们只看一第一个请求，你可以看到，有个Request URL，还有headers，下面便是response，这个头中包含了许许多多是信息，有文件编码啦，压缩方式啦，请求的agent啦等等。 其中，agent就是请求的身份，如果没有写入请求身份，那么服务器不一定会响应，所以可以在headers中设置agent,例如下面的例子，这个例子只是说明了怎样设置的headers，小伙伴们看一下设置格式就好。12345678910111213import urllibimport urllib2url = &apos;http://www.server.com/login&apos;user_agent = &apos;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&apos;values = &#123;&apos;username&apos; : &apos;cqc&apos;,&apos;password&apos; : &apos;XXXX&apos;&#125;headers = &#123; &apos;User-Agent&apos; : user_agent &#125;data = urllib.urlencode(values)request = urllib2.Request(url, data, headers)response = urllib2.urlopen(request)page = response.read() 这样，我们设置了一个headers，在构建request时传入，在请求时，就加入了headers传送，服务器若识别了是浏览器发来的请求，就会得到响应。 另外headers的一些属性，下面的需要特别注意一下：User-Agent : 有些服务器或 Proxy 会通过该值来判断是否是浏览器发出的请求Content-Type : 在使用 REST 接口时，服务器会检查该值，用来确定 HTTP Body 中的内容该怎样解析。application/xml ： 在 XML RPC，如 RESTful/SOAP 调用时使用application/json ： 在 JSON RPC 调用时使用application/x-www-form-urlencoded ： 浏览器提交 Web 表单时使用在使用服务器提供的 RESTful 或 SOAP 服务时， Content-Type 设置错误会导致服务器拒绝服务 Proxy（代理）的设置urllib2 默认会使用环境变量 http_proxy 来设置 HTTP Proxy。假如一个网站它会检测某一段时间某个IP 的访问次数，如果访问次数过多，它会禁止你的访问。所以你可以设置一些代理服务器来帮助你做工作，每隔一段时间换一个代理 创建一个代理处理器ProxyHandlerproxy_support = urllib.request.ProxyHandler()，ProxyHandler是一个类，其参数是一个字典：{ &#39;类型&#39;:&#39;代理ip:端口号&#39;} 定制、创建一个openeropener = urllib.request.build_opener(proxy_support) 使用openerurllib.request.install_opener(opener) opener.open(url) 从代理ip列表中随机使用某ip去访问URL的例子:1234567891011121314import urllib.requestimport randomurl = &apos;http://www.whatismyip.com.tw&apos;iplist = [&apos;115.32.41.100:80&apos;,&apos;58.30.231.36:80&apos;,&apos;123.56.90.175:3128&apos;]proxy_support = urllib.request.ProxyHandler(&#123;&apos;http&apos;:random.choice(iplist)&#125;)opener = urllib.request.build_opener(proxy_support)opener.addheaders = [(&apos;User-Agent&apos;,&apos;Test_Proxy_Python3.5_maminyao&apos;)]urllib.request.install_opener(opener)response = urllib.request.urlopen(url)html = response.read().decode(&apos;utf-8&apos;)print(html) Timeout 设置timeout的设置，可以设置等待多久超时，为了解决一些网站实在响应过慢而造成的影响。import urllib2 response = urllib2.urlopen(&#39;http://www.baidu.com&#39;, timeout=10)]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>urllib</tag>
      </tags>
  </entry>
</search>
