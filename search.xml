<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python 安装]]></title>
    <url>%2F2017%2F12%2F04%2FPython-anzhuang%2F</url>
    <content type="text"><![CDATA[python 环境准备安装 Python 首先从Python的官方网站python.org下载最新的2.7版本，网速慢的同学请移步国内镜像。 然后，运行下载的MSI安装包，在选择安装组件的一步时，勾上所有的组件： 特别要注意选上pip和Add python.exe to Path，然后一路点“Next”即可完成安装。 默认会安装到C:\Python27目录下，然后打开命令提示符窗口，敲入python后，会出现两种情况。情况一：看到上面的画面，就说明Python安装成功！你看到提示符&gt;&gt;&gt;就表示我们已经在Python交互式环境中了，可以输入任何Python代码，回车后会立刻得到执行结果。现在，输入exit()并回车，就可以退出Python交互式环境（直接关掉命令行窗口也可以！）。情况二：得到一个错误： ‘python’不是内部或外部命令，也不是可运行的程序或批处理文件。 这是因为Windows会根据一个Path的环境变量设定的路径去查找python.exe，如果没找到，就会报错。如果在安装时漏掉了勾选Add python.exe to Path，那就要手动把python.exe所在的路径C:\Python27添加到Path中。 如果你不知道怎么修改环境变量，建议把Python安装程序重新运行一遍，记得勾上Add python.exe to Path。安装 pycharm 首先从网站下载pycharm: http://www.jetbrains.com/pycharm/download/#section=windows，进入之后如下图，根据自己电脑的操作系统进行选择，对于windows系统选择图中红色圈中的区域。 下载完成之后如下图： 直接双击下载好的exe文件进行安装,点击Next进入下一步： 点击Install进行安装 安装完成后出现下图界面，点级Finish结束安装使用pycharm 单击桌面上的pycharm图标，进入到pycharm中 我们选择第二个，然后点击Ok 点击Accept进入下一步： 点击上图中的ok进入下一步：至此，安装完成]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python 安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python re]]></title>
    <url>%2F2017%2F12%2F04%2FPython-zhengze%2F</url>
    <content type="text"><![CDATA[#了解正则表达式 正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。正则表达式是用来匹配字符串非常强大的工具，在其他编程语言中同样有正则表达式的概念，Python同样不例外，利用了正则表达式，我们想要从返回的页面内容提取出我们想要的内容就易如反掌了。正则表达式的大致匹配过程是：1.依次拿出表达式和文本中的字符比较，2.如果每一个字符都能匹配，则匹配成功；一旦有匹配不成功的字符则匹配失败。3.如果表达式中有量词或边界，这个过程会稍微有一些不同。 正则表达式的语法规则正则表达式的语法规则 正则表达式相关注解正则表达式通常用于在文本中查找匹配的字符串。Python里数量词默认是贪婪的（在少数语言里也可能是默认非贪婪），总是尝试匹配尽可能多的字 符；非贪婪的则相反，总是尝试匹配尽可能少的字符。例如：正则表达式”ab”如果用于查找”abbbc”，将找到”abbb”。而如果使用非贪婪的数量 词”ab*?”，将找到”a”。注：我们一般使用非贪婪模式来提取。 反斜杠问题Python里的原生字符串很好地解决了这个问题，这个例子中的正则表达式可以使用r”\”表示。同样，匹配一个数字的”\d”可以写成r”\d”。有了原生字符串，妈妈也不用担心是不是漏写了反斜杠，写出来的表达式也更直观。 Python Re模块Python 自带了re模块，它提供了对正则表达式的支持。主要用到的方法列举如下：123456789101112import reA=&apos;12a88b22b368d8&apos; #查找所有的数字 +表示一次 或者多次 ？表示0次或者一次 *表示一次或者多次print re.findall(&apos;\d+&apos;,A)#查找所有的数字 返回一个对象obj_A=re.finditer(&apos;\d+&apos;,A)for i in obj_A: print i.group()#第一个参数为正则表达式 第二个参数想要替换成什么 第三个参数为替换的字符串print re.sub(r&apos;\d&apos;,&apos;z&apos;,A,2,flags=re.I)re.purge() #清空正则表达式缓存 re.split(pattern, string[, maxsplit])按照能够匹配的子串将string分割后返回列表。maxsplit用于指定最大分割次数，不指定将全部分割。1234567import repattern = re.compile(r&apos;\d+&apos;)print re.split(pattern,&apos;one1two2three3four4&apos;)### 输出 #### [&apos;one&apos;, &apos;two&apos;, &apos;three&apos;, &apos;four&apos;, &apos;&apos;] re.findall(pattern, string[, flags])搜索string，以列表形式返回全部能匹配的子串。1234567import repattern = re.compile(r&apos;\d+&apos;)print re.findall(pattern,&apos;one1two2three3four4&apos;)### 输出 #### [&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;4&apos;] re.sub(pattern, repl, string[, count])使用repl替换string中每一个匹配的子串后返回替换后的字符串。当repl是一个字符串时，可以使用\id或\g、\g引用分组，但不能使用编号0。当repl是一个方法时，这个方法应当只接受一个参数（Match对象），并返回一个字符串用于替换（返回的字符串中不能再引用分组）。count用于指定最多替换次数，不指定时全部替换。123456789101112131415import repattern = re.compile(r&apos;(\w+) (\w+)&apos;)s = &apos;i say, hello world!&apos;print re.sub(pattern,r&apos;\2 \1&apos;, s)def func(m): return m.group(1).title() + &apos; &apos; + m.group(2).title()print re.sub(pattern,func, s)### output #### say i, world hello!# I Say, Hello World!]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python 安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 数据类型]]></title>
    <url>%2F2017%2F12%2F04%2FPython-shujuleixing%2F</url>
    <content type="text"><![CDATA[数据类型在Python中，能够直接处理的数据类型有以下几种： 基本数据类型整数Python可以处理任意大小的整数，当然包括负整数，在程序中的表示方法和数学上的写法一模一样，例如：1，100，-8080，0，等等。计算机由于使用二进制，所以，有时候用十六进制表示整数比较方便，十六进制用0x前缀和0-9，a-f表示，例如：0xff00，0xa5b4c3d2，等等。 浮点数浮点数也就是小数，之所以称为浮点数，是因为按照科学记数法表示时，一个浮点数的小数点位置是可变的，浮点数可以用数学写法，如1.23，3.14，-9.01，等等。但是对于很大或很小的浮点数，就必须用科学计数法表示，把10用e替代，1.23x109就是1.23e9，或者12.3e8，0.000012可以写成1.2e-5，等等。 字符串字符串是以’’或””括起来的任意文本，比如&#39;abc&#39;，&quot;xyz&quot;等等。请注意，&#39;&#39;或&quot;&quot;本身只是一种表示方式，不是字符串的一部分，因此，字符串&#39;abc&#39;只有a，b，c这3个字符。如果&#39;本身也是一个字符，那就可以用&quot;&quot;括起来，比如&quot;I&#39;m OK&quot;包含的字符是I，&#39;，m，空格，O，K这6个字符。如果字符串内部既包含&#39;又包含&quot;怎么办？可以用转义字符\来标识，比如：&#39;I\&#39;m \&quot;OK\&quot;!&#39;表示的字符串内容是：I&#39;m &quot;OK&quot;! 布尔值布尔值和布尔代数的表示完全一致，一个布尔值只有True、False两种值，要么是True，要么是False，在Python中，可以直接用True、False表示布尔值（请注意大小写），也可以通过布尔运算计算出来：12345678print TrueTrueprint FalseFalseprint 3 &gt; 2Trueprint 3 &gt; 5False 布尔值可以用and、or和not运算。 空值空值是Python里一个特殊的值，用None表示。None不能理解为0，因为0是有意义的，而None是一个特殊的空值。 变量变量名必须是大小写英文、数字和_的组合，且不能用数字开头 数据类型进阶列表（list）Python内置的一种数据类型是列表：list。list是一种有序的集合，可以随时添加和删除其中的元素。声明list：list=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]用索引来访问list中每一个位置的元素，记得索引是从0开始的例如list [0]如果要取最后一个元素，除了计算索引位置外，还可以用-1做索引，直接获取最后一个元素：list[-1]list是一个可变的有序表，所以，可以往list中追加元素到末尾：list.append([指定位置]&#39;aaa&#39;)要删除list的元素，用pop()方法：list.pop([指定位置默认为最后])要把某个元素替换成别的元素，可以直接赋值给对应的索引位置,list里面的元素的数据类型也可以不同。list元素也可以是另一个list,例如：s = [&#39;python&#39;, &#39;java&#39;, [&#39;asp&#39;, &#39;php&#39;], &#39;scheme&#39;] 元祖（tuple）另一种有序列表叫元组：tuple。tuple和list非常类似，但是tuple一旦初始化就不能修改声明元祖t=（1，2）只有1个元素的tuple定义时必须加一个逗号,，来消除歧义：t=(1,)最后来看一个“可变的”tuple：123t = (&apos;a&apos;, &apos;b&apos;, [&apos;A&apos;, &apos;B&apos;])t[2][0] = &apos;X&apos;t[2][0] = &apos;X&apos; 字典（dict）Python内置了字典：dict的支持，dict全称dictionary，在其他语言中也称为map，使用键-值（key-value）存储，具有极快的查找速度。声明dict d = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3} setset和dict类似，也是一组key的集合，但不存储value。由于key不能重复，所以，在set中，没有重复的key。要创建一个set，需要提供一个list作为输入集合:声明 set s = set([1,2,3])set可以看成数学意义上的无序和无重复元素的集合，因此，两个set可以做数学意义上的交集、并集等操作：123456print s1 = set([1, 2, 3])print s2 = set([2, 3, 4])print s1 &amp; s2set([2, 3])print s1 | s2set([1, 2, 3, 4])]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>-python 语法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python beautifulsoup]]></title>
    <url>%2F2017%2F12%2F04%2FPython-parser-bs4%2F</url>
    <content type="text"><![CDATA[Beautiful Soup的简介简单来说，BeautifulSoup是python的一个库，最主要的功能是从网页抓取数据。官方解释如下： Beautiful Soup提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为utf-8编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时，Beautiful Soup就不能自动识别编码方式了。然后，你仅仅需要说明一下原始编码方式就可以了。Beautiful Soup已成为和lxml、html6lib一样出色的python解释器，为用户灵活地提供不同的解析策略或强劲的速度。 Beautiful Soup 安装Beautiful Soup 3 目前已经停止开发，推荐在现在的项目中使用Beautiful Soup 4，不过它已经被移植到BS4了，也就是说导入时我们需要 import bs4 。pycharm 有非常强大的python库，所有常用的库都可以通过pycharm的Default Settings来下载,如图： 第一步 第二步 第三步 这样就安装完成了 开启Beautiful Soup 之旅在这里先分享官方文档链接，不过内容是有些多，也不够条理，在此本文章做一下整理方便大家参考 https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html 创建 Beautiful Soup 对象首先必须要导入 bs4 库1from bs4 import BeautifulSoup 我们创建一个字符串，后面的例子我们便会用它来演示:1234567891011html = &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;&quot;&quot;&quot; 创建 beautifulsoup 对象12soup = BeautifulSoup(html)print soup.prettify() 四大对象种类Beautiful Soup将复杂HTML文档转换成一个复杂的树形结构,每个节点都是Python对象,所有对象可以归纳为4种: Tag NavigableString BeautifulSoup Comment 搜索文档树find_all( name , attrs , recursive , text , **kwargs )find_all() 方法搜索当前tag的所有tag子节点,并判断是否符合过滤器的条件 name 参数name 参数可以查找所有名字为 name 的tag,字符串对象会被自动忽略掉 传字符串：最简单的过滤器是字符串.在搜索方法中传入一个字符串参数,Beautiful Soup会查找与字符串完整匹配的内容,下面的例子用于查找文档中所有的&lt;b&gt;标签soup.find_all(&#39;b&#39;) 传正则表达式如果传入正则表达式作为参数,Beautiful Soup会通过正则表达式的 match() 来匹配内容.下面例子中找出所有以b开头的标签,这表示&lt;body&gt;和&lt;b&gt;标签都应该被找到12345import refor tag in soup.find_all(re.compile(&quot;^b&quot;)): print(tag.name)# body# b 传列表如果传入列表参数,Beautiful Soup会将与列表中任一元素匹配的内容返回.下面代码找到文档中所有标签和&lt;b&gt;标签12345soup.find_all([&quot;a&quot;, &quot;b&quot;])# [&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;,# &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;] 传 TrueTrue 可以匹配任何值,下面代码查找到所有的tag,但是不会返回字符串节点1234567891011for tag in soup.find_all(True): print(tag.name)# html# head# title# body# p# b# p# a# a keyword 参数注意：如果一个指定名字的参数不是搜索内置的参数名,搜索时会把该参数当作指定名字tag的属性来搜索,如果包含一个名字为 id 的参数,Beautiful Soup会搜索每个tag的”id”属性12soup.find_all(id=&apos;link2&apos;)# [&lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt;] 如果传入 href 参数,Beautiful Soup会搜索每个tag的href属性12soup.find_all(href=re.compile(&quot;elsie&quot;))# [&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;] 在这里我们想用 class 过滤，不过 class 是 python 的关键词，这怎么办？加个下划线就可以soup.find_all(&quot;a&quot;, class_=&quot;sister&quot;) text 参数通过 text 参数可以搜搜文档中的字符串内容.与 name 参数的可选值一样, text 参数接受 字符串 , 正则表达式 , 列表, True12345678soup.find_all(text=&quot;Elsie&quot;)# [u&apos;Elsie&apos;]soup.find_all(text=[&quot;Tillie&quot;, &quot;Elsie&quot;, &quot;Lacie&quot;])# [u&apos;Elsie&apos;, u&apos;Lacie&apos;, u&apos;Tillie&apos;]soup.find_all(text=re.compile(&quot;Dormouse&quot;))#[u&quot;The Dormouse&apos;s story&quot;, u&quot;The Dormouse&apos;s story&quot;] recursive 参数调用tag的 find_all() 方法时,Beautiful Soup会检索当前tag的所有子孙节点,如果只想搜索tag的直接子节点,可以使用参数 recursive=False find( name , attrs , recursive , text , **kwargs )它与 find_all() 方法唯一的区别是 find_all() 方法的返回结果是值包含一个元素的列表,而 find() 方法直接返回结果 CSS选择器我们在写 CSS 时，标签名不加任何修饰，类名前加点，id名前加 #，在这里我们也可以利用类似的方法来筛选元素，用到的方法是 soup.select()，返回类型是 list 通过标签名查找12345678print soup.select(&apos;title&apos;)#[&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;]print soup.select(&apos;a&apos;)#[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;]print soup.select(&apos;b&apos;)#[&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;] 通过类名查找 加.12print soup.select(&apos;.sister&apos;)#[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; 通过 id 名查找 加12print soup.select(&apos;#link1&apos;)#[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; 组合查找组合查找即和写 class 文件时，标签名与类名、id名进行的组合原理是一样的，例如查找 p 标签中，id 等于 link1的内容，二者需要用空格分开12print soup.select(&apos;p #link1&apos;)#[&lt;p class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt; 属性查找查找时还可以加入属性元素，属性需要用中括号括起来，注意属性和标签属于同一节点，所以中间不能加空格，否则会无法匹配到。12345print soup.select(&apos;a[href=&quot;http://example.com/elsie&quot;]&apos;)#[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;print soup.select(&apos;p a[href=&quot;http://example.com/elsie&quot;]&apos;)#[&lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>bs4</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java]]></title>
    <url>%2F2017%2F12%2F04%2FJava%2F</url>
    <content type="text"></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫认知]]></title>
    <url>%2F2017%2F12%2F04%2FPython-parser-jichu%2F</url>
    <content type="text"><![CDATA[综述大家好，最近博主在学习Python，学习期间也遇到一些问题，获得了一些经验，在此将自己的学习系统地整理下来，如果大家有兴趣学习爬虫的话，可以将这些文章作为参考，也欢迎大家一共分享学习经验。 Python版本:2.7，Python 3请另寻其他博文。 根据我的经验，要学习Python爬虫，我们要学习的共有以下几点： Python基础知识 Python中urllib和urllib2库的用法 Python正则表达式 Python爬虫框架Scrapy Python爬虫更高级的功能 Python基础学习首先，我们要用Python写爬虫，肯定要了解Python的基础吧，万丈高楼平地起，不能忘啦那地基，哈哈，那么我就分享一下自己曾经看过的一些Python教程，小伙伴们可以作为参考。 慕课网Python教程曾经有一些基础的语法是在慕课网上看的，上面附有一些练习，学习完之后可以作为练习，感觉效果还是蛮不错的，不过稍微遗憾的是内容基本上都是最基础的，入门开始的话，就这个吧 学习网址：https://www.imooc.com/course/list?c=python 廖雪峰Python教程后来，我发现了廖老师的Python教程，讲的那是非常通俗易懂哪，感觉也是非常不错，大家如果想进一步了解Python就看一下这个吧。 学习网址：https://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000 Python urllib和urllib2 库的用法urllib和urllib2库是学习Python爬虫最基本的库，利用这个库我们可以得到网页的内容，并对内容用正则表达式提取分析，得到我们想要的结果。这个在学习过程中我会和大家分享的。 Python 正则表达式Python正则表达式是一种用来匹配字符串的强有力的武器。它的设计思想是用一种描述性的语言来给字符串定义一个规则，凡是符合规则的字符串，我们就认为它“匹配”了，否则，该字符串就是不合法的。这个在后面的博文会分享的。 爬虫框架Scrapy如果你是一个Python高手，基本的爬虫知识都已经掌握了，那么就寻觅一下Python框架吧，我选择的框架是Scrapy框架。这个框架有什么强大的功能呢？下面是它的官方介绍： HTML, XML源数据 选择及提取 的内置支持提供了一系列在spider之间共享的可复用的过滤器(即 Item Loaders)，对智能处理爬取数据提供了内置支持。通过 feed导出 提供了多格式(JSON、CSV、XML)，多存储后端(FTP、S3、本地文件系统)的内置支持提供了media pipeline，可以 自动下载 爬取到的数据中的图片(或者其他资源)。高扩展性。您可以通过使用 signals ，设计好的API(中间件, extensions, pipelines)来定制实现您的功能。内置的中间件及扩展为下列功能提供了支持:cookies and session 处理HTTP 压缩HTTP 认证HTTP 缓存user-agent模拟robots.txt爬取深度限制针对非英语语系中不标准或者错误的编码声明, 提供了自动检测以及健壮的编码支持。支持根据模板生成爬虫。在加速爬虫创建的同时，保持在大型项目中的代码更为一致。详细内容请参阅 genspider 命令。针对多爬虫下性能评估、失败检测，提供了可扩展的 状态收集工具 。提供 交互式shell终端 , 为您测试XPath表达式，编写和调试爬虫提供了极大的方便提供 System service, 简化在生产环境的部署及运行内置 Web service, 使您可以监视及控制您的机器内置 Telnet终端 ，通过在Scrapy进程中钩入Python终端，使您可以查看并且调试爬虫Logging 为您在爬取过程中捕捉错误提供了方便支持 Sitemaps 爬取具有缓存的DNS解析器 官方文档：http://doc.scrapy.org/en/latest/ 爬虫基础了解什么是爬虫爬虫，即网络爬虫，大家可以理解为在网络上爬行的一直蜘蛛，互联网就比作一张大网，而爬虫便是在这张网上爬来爬去的蜘蛛咯，如果它遇到资源，那么它就会抓取下来。想抓取什么？这个由你来控制它咯。比如它在抓取一个网页，在这个网中他发现了一条道路，其实就是指向网页的超链接，那么它就可以爬到另一张网上来获取数据。这样，整个连在一起的大网对这之蜘蛛来说触手可及，分分钟爬下来不是事儿。 浏览网页的过程在用户浏览网页的过程中，我们可能会看到许多好看的图片，比如 http://image.baidu.com/ ，我们会看到几张的图片以及百度搜索框，这个过程其实就是用户输入网址之后，经过DNS服务器，找到服务器主机，向服务器发出一个请求，服务器经过解析之后，发送给用户的浏览器 HTML、JS、CSS 等文件，浏览器解析出来，用户便可以看到形形色色的图片了。 URL的含义URL，即统一资源定位符，也就是我们说的网址，统一资源定位符是对可以从互联网上得到的资源的位置和访问方法的一种简洁的表示，是互联网上标准资源的地址。互联网上的每个文件都有一个唯一的URL，它包含的信息指出文件的位置以及浏览器应该怎么处理它。URL的格式由三部分组成： ①第一部分是协议(或称为服务方式)。 ②第二部分是存有该资源的主机IP地址(有时也包括端口号)。 ③第三部分是主机资源的具体地址，如目录和文件名等。爬虫爬取数据时必须要有一个目标的URL才可以获取数据，因此，它是爬虫获取数据的基本依据，准确理解它的含义对爬虫学习有很大帮助。 环境的配置学习Python，当然少不了环境的配置，最初我用的是Sublime，不过发现它没有提示功能实在是太弱了，于是，在Windows下我用了 PyCharm.]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Urllib库]]></title>
    <url>%2F2017%2F12%2F04%2FPython-parser-jinjie%2F</url>
    <content type="text"><![CDATA[urllib库的使用扒一个网页怎样扒网页呢？其实就是根据URL来获取它的网页信息，虽然我们在浏览器中看到的是一幅幅优美的画面，但是其实是由浏览器解释才呈现出来的，实质它 是一段HTML代码，加 JS、CSS，如果把网页比作一个人，那么HTML便是他的骨架，JS便是他的肌肉，CSS便是它的衣服。所以最重要的部分是存在于HTML中的，下面我 们就写个例子来扒一个网页下来。在pycharm里新建一个.py的文件名字叫firstparser.py1234import urllib2response = urllib2.urlopen(&quot;http://www.baidu.com&quot;)print response.read() 是的你没看错，真正的程序就两行，右键选择 run ，查看运行结果，感受一下:看，这个网页的源码已经被我们扒下来了，是不是很酸爽？ 分析扒网页的方法那么我们来分析这两行代码，第一行: response = urllib2.urlopen(&quot;http://www.baidu.com&quot;) 首先我们调用的是urllib2库里面的urlopen方法，传入一个URL，这个网址是百度首页，协议是HTTP协议，当然你也可以把HTTP换做FTP,FILE,HTTPS 等等，只是代表了一种访问控制协议，urlopen一般接受三个参数，它的参数如下： urlopen(url, data, timeout) 第一个参数url即为URL，第二个参数data是访问URL时要传送的数据，第三个timeout是设置超时时间。 第二三个参数是可以不传送的，data默认为空None，timeout默认为 socket._GLOBAL_DEFAULT_TIMEOUT 第一个参数URL是必须要传送的，在这个例子里面我们传送了百度的URL，执行urlopen方法之后，返回一个response对象，返回信息便保存在这里面。 print response.read() response对象有一个read方法，可以返回获取到的网页内容。 构造Requset其实上面的urlopen参数可以传入一个request请求,它其实就是一个Request类的实例，构造时需要传入Url,Data等等的内容。比如上面的两行代码，我们可以这么改写:12345import urllib2request = urllib2.Request(&quot;http://www.baidu.com&quot;)response = urllib2.urlopen(request)print response.read() 运行结果是完全一样的，只不过中间多了一个request对象，推荐大家这么写，因为在构建请求时还需要加入好多内容，通过构建一个request，服务器响应请求得到应答，这样显得逻辑上清晰明确。 POST和GET数据传送上面的程序演示了最基本的网页抓取，不过，现在大多数网站都是动态网页，需要你动态地传递参数给它，数据传送分为POST和GET两种方式: POST我们传送的数据就是这个参数data，下面演示一下POST方式。123456789import urllibimport urllib2values = &#123;&quot;username&quot;:&quot;1016903103@qq.com&quot;,&quot;password&quot;:&quot;XXXX&quot;&#125;data = urllib.urlencode(values)url = &quot;https://passport.csdn.net/account/login?from=http://my.csdn.net/my/mycsdn&quot;request = urllib2.Request(url,data)response = urllib2.urlopen(request)print response.read() 在此只是说明登录的原理。我们需要定义一个字典，名字为values，参数我设置了username和 password，下面利用urllib的urlencode方法将字典编码，命名为data，构建request时传入两个参数，url和data，运 行程序，即可实现登陆，返回的便是登陆后呈现的页面内容。当然你可以自己搭建一个服务器来测试一下。 GET方式至于GET方式我们可以直接把参数写到网址上面，直接构建一个带参数的URL出来即可。123456789101112import urllibimport urllib2values=&#123;&#125;values[&apos;username&apos;] = &quot;1016903103@qq.com&quot;values[&apos;password&apos;]=&quot;XXXX&quot;data = urllib.urlencode(values)url = &quot;http://passport.csdn.net/account/login&quot;geturl = url + &quot;?&quot;+datarequest = urllib2.Request(geturl)response = urllib2.urlopen(request)print response.read() 你可以print geturl，打印输出一下url，发现其实就是原来的url加？然后加编码后的参数1http://passport.csdn.net/account/login?username=1016903103%40qq.com&amp;password=XXXX urllib库的扩展设置Headers有些网站不会同意程序直接用上面的方式进行访问，如果识别有问题，那么站点根本不会响应，所以为了完全模拟浏览器的工作，我们需要设置一些Headers 的属性。 首先，打开我们的浏览器，调试浏览器F12，我用的是Chrome，打开网络监听经过多次请求之后，网页的骨架和肌肉全了，整个网页的效果也就出来了。 拆分这些请求，我们只看一第一个请求，你可以看到，有个Request URL，还有headers，下面便是response，这个头中包含了许许多多是信息，有文件编码啦，压缩方式啦，请求的agent啦等等。 其中，agent就是请求的身份，如果没有写入请求身份，那么服务器不一定会响应，所以可以在headers中设置agent,例如下面的例子，这个例子只是说明了怎样设置的headers，小伙伴们看一下设置格式就好。12345678910111213import urllibimport urllib2url = &apos;http://www.server.com/login&apos;user_agent = &apos;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&apos;values = &#123;&apos;username&apos; : &apos;cqc&apos;,&apos;password&apos; : &apos;XXXX&apos;&#125;headers = &#123; &apos;User-Agent&apos; : user_agent &#125;data = urllib.urlencode(values)request = urllib2.Request(url, data, headers)response = urllib2.urlopen(request)page = response.read() 这样，我们设置了一个headers，在构建request时传入，在请求时，就加入了headers传送，服务器若识别了是浏览器发来的请求，就会得到响应。 另外headers的一些属性，下面的需要特别注意一下：User-Agent : 有些服务器或 Proxy 会通过该值来判断是否是浏览器发出的请求Content-Type : 在使用 REST 接口时，服务器会检查该值，用来确定 HTTP Body 中的内容该怎样解析。application/xml ： 在 XML RPC，如 RESTful/SOAP 调用时使用application/json ： 在 JSON RPC 调用时使用application/x-www-form-urlencoded ： 浏览器提交 Web 表单时使用在使用服务器提供的 RESTful 或 SOAP 服务时， Content-Type 设置错误会导致服务器拒绝服务 Proxy（代理）的设置urllib2 默认会使用环境变量 http_proxy 来设置 HTTP Proxy。假如一个网站它会检测某一段时间某个IP 的访问次数，如果访问次数过多，它会禁止你的访问。所以你可以设置一些代理服务器来帮助你做工作，每隔一段时间换一个代理 创建一个代理处理器ProxyHandlerproxy_support = urllib.request.ProxyHandler()，ProxyHandler是一个类，其参数是一个字典：{ &#39;类型&#39;:&#39;代理ip:端口号&#39;} 定制、创建一个openeropener = urllib.request.build_opener(proxy_support) 使用openerurllib.request.install_opener(opener) opener.open(url) 从代理ip列表中随机使用某ip去访问URL的例子:1234567891011121314import urllib.requestimport randomurl = &apos;http://www.whatismyip.com.tw&apos;iplist = [&apos;115.32.41.100:80&apos;,&apos;58.30.231.36:80&apos;,&apos;123.56.90.175:3128&apos;]proxy_support = urllib.request.ProxyHandler(&#123;&apos;http&apos;:random.choice(iplist)&#125;)opener = urllib.request.build_opener(proxy_support)opener.addheaders = [(&apos;User-Agent&apos;,&apos;Test_Proxy_Python3.5_maminyao&apos;)]urllib.request.install_opener(opener)response = urllib.request.urlopen(url)html = response.read().decode(&apos;utf-8&apos;)print(html) Timeout 设置timeout的设置，可以设置等待多久超时，为了解决一些网站实在响应过慢而造成的影响。import urllib2 response = urllib2.urlopen(&#39;http://www.baidu.com&#39;, timeout=10)]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>urllib</tag>
      </tags>
  </entry>
</search>
